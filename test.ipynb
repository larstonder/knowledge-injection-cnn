{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode\n",
      "Directory already exists\n",
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "# Author: Mattia Silvestri\n",
    "\n",
    "\"\"\"\n",
    "Main program.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "from utility import PLSInstance, PLSSolver, random_assigner\n",
    "from models import MyModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import argparse\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Set seed in order to reproduce results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Tensorflow 2 GPU setup\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dim\", type=int, default=10, help=\"Problem dimension\")\n",
    "parser.add_argument(\"--train\", action=\"store_true\",\n",
    "                    help=\"Train the model; if not set the default is test mode.\", default=False)\n",
    "parser.add_argument(\"--test-num\", default=None,\n",
    "                    help=\"Test identifier.\")\n",
    "parser.add_argument(\"--num-epochs\", default=300, type=int,\n",
    "                    help=\"Number of training epochs.\")\n",
    "parser.add_argument(\"--max-size\", default=1000000, type=int,\n",
    "                    help=\"Maximum number of training/test instances to be loaded.\")\n",
    "parser.add_argument(\"--load-mode\", default=\"onehot\", choices=[\"onehot\", \"string\"],\n",
    "                    help=\"Dataset loading mode.\")\n",
    "parser.add_argument(\"--batch-size\", default=1024, type=int,\n",
    "                    help=\"Mini-batch size.\")\n",
    "parser.add_argument(\"--leave-columns-domains\", action=\"store_true\", default=False,\n",
    "                    help=\"True if you don't want to prune columns domains values with forward checking.\")\n",
    "parser.add_argument(\"--num-sol\", type=str, default=\"10k\",\n",
    "                    help=\"Number of solutions from which the training set has been generated; thousands are expressed \"\n",
    "                         + \"with k (for example 10000=10k).\")\n",
    "parser.add_argument(\"--model\", default=\"fnn\", choices=[\"fnn\", \"cnn\"],\n",
    "                    help=\"Choose the model architecture.\")  # TODO: change default\n",
    "parser.add_argument(\"--model-type\", default=\"agnostic\", choices=[\"agnostic\", \"sbrinspiredloss\", \"negative\", \"binary\"],\n",
    "                    help=\"Choose the model type. 'agnostic' is the model-agnostic baseline. 'sbrinspiredloss', \"\n",
    "                         + \"'negative' and 'binary' are relatively the mse, negative and binary-cross entropy versions\"\n",
    "                         + \" of the SBR inspiredloss.\")\n",
    "parser.add_argument(\"--validation-size\", type=int, default=0,\n",
    "                    help=\"Validation set dimension. If zero no validation set is used.\")\n",
    "parser.add_argument(\"--use-prop\", action=\"store_true\", default=False,\n",
    "                    help=\"True if you want to assist the estimators with constraints propagation at evaluation time.\")\n",
    "parser.add_argument(\"--rnd-feas\", action=\"store_true\", default=False,\n",
    "                    help=\"True if you want to compute feasibility ratio also for random estimator.\")\n",
    "parser.add_argument(\"--lmbd\", default=1.0, type=float, help=\"Lambda for SBR-inspired term.\")\n",
    "parser.add_argument(\"--patience\", default=10, type=int,\n",
    "                    help=\"Specify the number of 10 epochs intervals without improvement in \"\n",
    "                         \"feasibility after which training is stopped.\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args([\"--dim\", \"7\", \"--train\", \"--test-num\", \"pls-7/cnn/all-ts/run-1\", \"--num-epochs\", \"10000\", \"--max-size\", \"1000000\", \"--batch-size\", \"2048\", \"--num-sol\", \"10k\", \"--model\", \"cnn\", \"--model-type\", \"agnostic\", \"--validation-size\", \"5000\", \"--patience\", \"10\"])\n",
    "\n",
    "# Problem dimension.\n",
    "DIM = int(args.dim)\n",
    "\n",
    "COLUMN_TYPES = [int() for _ in range(DIM ** 3)]\n",
    "\n",
    "# Set training or test mode.\n",
    "TRAIN = args.train\n",
    "if TRAIN:\n",
    "    print(\"Training mode\")\n",
    "    mode = \"train\"\n",
    "else:\n",
    "    print(\"Test mode\")\n",
    "    mode = \"test\"\n",
    "\n",
    "# Test number identifier\n",
    "TEST_NUM = args.test_num\n",
    "\n",
    "# Number of training epochs\n",
    "EPOCHS = int(args.num_epochs)\n",
    "\n",
    "# Maximum number of data set examples to load\n",
    "MAX_SIZE = int(args.max_size)\n",
    "\n",
    "# Available loading mode are string and one-hot\n",
    "LOAD_MODE = args.load_mode\n",
    "\n",
    "# Mini-batch size\n",
    "BATCH_SIZE = int(args.batch_size)\n",
    "\n",
    "# True if you want to adopt SRB-inspired loss function\n",
    "MODEL_TYPE = args.model_type\n",
    "\n",
    "if mode == \"test\":\n",
    "    mode_char = \"L\"\n",
    "else:\n",
    "    mode_char = \"B\"\n",
    "\n",
    "if not TRAIN:\n",
    "    file_name = \"pls{}_10k\".format(DIM)\n",
    "else:\n",
    "    file_name = \"pls{}_{}\".format(DIM, args.num_sol)\n",
    "\n",
    "VAL_SIZE = args.validation_size\n",
    "\n",
    "NUM_SOL = args.num_sol\n",
    "\n",
    "# Model name for both training and test\n",
    "model_name = \"test-{}/\".format(TEST_NUM)\n",
    "\n",
    "# Where to save plots\n",
    "SAVE_PATH = \"plots/test-{}/\".format(TEST_NUM)\n",
    "try:\n",
    "    os.makedirs(SAVE_PATH)\n",
    "except:\n",
    "    print(\"Directory already exists\")\n",
    "\n",
    "# Model name for both training and test\n",
    "model_name = \"test-{}/\".format(TEST_NUM)\n",
    "\n",
    "# Where to save plots\n",
    "SAVE_PATH = \"plots/test-{}/\".format(TEST_NUM)\n",
    "try:\n",
    "    os.makedirs(SAVE_PATH)\n",
    "except:\n",
    "    print(\"Directory already exists\")\n",
    "\n",
    "########################################################################################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation set...\n",
      "Elapsed 11.437053918838501 seconds\n",
      "Loading features from datasets/pls7/partial_solutions_10k_train.csv...\n",
      "Elapsed 6.831673860549927 seconds, 0.122217074 GB required\n",
      "Number of rows: 356318\n",
      "Loading labels from datasets/pls7/assignments_10k_train.csv...\n",
      "Elapsed 0.19371724128723145 seconds, 0.001425272 GB required\n",
      "Elapsed 0.23100924491882324 seconds, 0.122217074 GB required\n"
     ]
    }
   ],
   "source": [
    "# Create a validation set if required\n",
    "val_indexes = None\n",
    "\n",
    "if VAL_SIZE > 0:\n",
    "    print(\"Loading validation set...\")\n",
    "    start = time.time()\n",
    "    X_val = pd.read_csv(\"datasets/pls{}/partial_solutions_{}_train.csv\".format(DIM, NUM_SOL),\n",
    "                        sep=',',\n",
    "                        header=None,\n",
    "                        nrows=MAX_SIZE,\n",
    "                        dtype=np.int8).values\n",
    "\n",
    "    # Create penalties for the examples\n",
    "    if MODEL_TYPE != 'agnostic':\n",
    "        P_val = pd.read_csv(\"datasets/pls{}/domains_train_{}.csv\".format(DIM, NUM_SOL),\n",
    "                            sep=',',\n",
    "                            header=None,\n",
    "                            nrows=MAX_SIZE,\n",
    "                            dtype=np.int8).values\n",
    "    else:\n",
    "        P_val = np.zeros_like(X_val, dtype=np.int8)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Elapsed {} seconds\".format((end - start)))\n",
    "\n",
    "    val_indexes = np.random.choice(np.arange(0, X_val.shape[0]), size=VAL_SIZE, replace=False)\n",
    "    X_val = X_val[val_indexes]\n",
    "    P_val = P_val[val_indexes]\n",
    "    validation_set = (X_val, P_val)\n",
    "\n",
    "# Load training examples\n",
    "features_filepath = \"datasets/pls{}/partial_solutions_{}_{}.csv\".format(DIM, NUM_SOL, mode)\n",
    "print(\"Loading features from {}...\".format(features_filepath))\n",
    "start = time.time()\n",
    "X = pd.read_csv(features_filepath, sep=',', header=None, nrows=MAX_SIZE, dtype=np.int8).values\n",
    "end = time.time()\n",
    "print(\"Elapsed {} seconds, {} GB required\".format((end - start), X.nbytes / 10 ** 9))\n",
    "print(\"Number of rows: {}\".format(X.shape[0]))\n",
    "\n",
    "labels_filepath = \"datasets/pls{}/assignments_{}_{}.csv\".format(DIM, NUM_SOL, mode)\n",
    "print(\"Loading labels from {}...\".format(labels_filepath))\n",
    "start = time.time()\n",
    "Y = pd.read_csv(labels_filepath, sep=',', header=None, nrows=MAX_SIZE, dtype=np.int32).values\n",
    "end = time.time()\n",
    "print(\"Elapsed {} seconds, {} GB required\".format((end - start), Y.nbytes / 10 ** 9))\n",
    "\n",
    "# Create penalties for the examples\n",
    "if MODEL_TYPE == 'agnostic' and not args.use_prop:\n",
    "    P = np.zeros_like(X, dtype=np.int8)\n",
    "else:\n",
    "    if not args.leave_columns_domains:\n",
    "        penalties_filepath = \"datasets/pls{}/domains_{}_{}.csv\".format(DIM, mode, NUM_SOL)\n",
    "    else:\n",
    "        penalties_filepath = \"datasets/pls{}/rows_propagation_domains_{}_{}.csv\".format(DIM, mode, NUM_SOL)\n",
    "\n",
    "    print(\"Loading penalties from {}...\".format(penalties_filepath))\n",
    "    start = time.time()\n",
    "    P = pd.read_csv(penalties_filepath, sep=',', header=None, nrows=MAX_SIZE, dtype=np.int8).values\n",
    "end = time.time()\n",
    "print(\"Elapsed {} seconds, {} GB required\".format((end - start), P.nbytes / 10 ** 9))\n",
    "\n",
    "# Remove validation samples from the training set\n",
    "if val_indexes is not None:\n",
    "    X = np.delete(X, val_indexes, axis=0)\n",
    "    Y = np.delete(Y, val_indexes, axis=0)\n",
    "    P = np.delete(P, val_indexes, axis=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "# Create TF datasets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y, P)).shuffle(10000).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "####### MODEL DEFINITION - models.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Reshape\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "class PLSCNNModel(MyModel):\n",
    "    def _define_model(self, input_shape):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(Reshape((7, 7, 7), input_shape=(7**3,)))\n",
    "\n",
    "        # model.add(Conv2D(32, kernel_size=(7, 1), activation='relu', padding='same'))\n",
    "        # model.add(Conv2D(32, kernel_size=(1, 7), activation='relu', padding='same'))\n",
    "        # # model.add(MaxPooling2D((2, 2)))\n",
    "        # model.add(BatchNormalization())\n",
    "        #\n",
    "        # model.add(Conv2D(64, (3, 1), activation='relu', padding='same'))\n",
    "        # model.add(Conv2D(64, (1, 3), activation='relu', padding='same'))\n",
    "        # # model.add(MaxPooling2D((2, 2)))\n",
    "        # model.add(BatchNormalization())\n",
    "        #\n",
    "        # model.add(Conv2D(128, (1, 1), activation='relu', padding='same'))\n",
    "        # model.add(BatchNormalization())\n",
    "        #\n",
    "        # model.add(Flatten())\n",
    "        # model.add(Dense(64, activation='relu'))\n",
    "        # model.add(Dense(7**3))  # Loss function expects logits\n",
    "\n",
    "\n",
    "\n",
    "        from tensorflow.keras import layers\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "        def conv2D_bn_relu(x, filters, kernel_size, strides=(1, 1), padding='same', kernel_initializer='glorot_uniform', name=None):\n",
    "            \"\"\"2D convolution layer followed by batch normalization and ReLU activation.\"\"\"\n",
    "            x = layers.Conv2D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              strides=strides,\n",
    "                              padding=padding,\n",
    "                              kernel_initializer=kernel_initializer,\n",
    "                              name=name,\n",
    "                              use_bias=False)(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            return layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "        def inception_module(x, filters=None, kernel_initializer='glorot_uniform'):\n",
    "            \"\"\"Variant of Inception module as described in Figure 6 of \"Going deeper with convolutions\" (Szegedy, et al. 2014).\n",
    "\n",
    "            # Arguments\n",
    "                :param x - 4D tensor with shape: `(batch, rows, columns, channels)`.\n",
    "                :param filters - Number of output filters for the module.\n",
    "                :param kernel_initializer: Weight initializer for all convolutional layers in module.\n",
    "            \"\"\"\n",
    "\n",
    "            b1 = conv2D_bn_relu(x,\n",
    "                                 filters=filters,\n",
    "                                 kernel_size=(3, 1),\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 name='conv__6_1_3x1',\n",
    "                                 kernel_initializer=kernel_initializer)\n",
    "\n",
    "            b1 = conv2D_bn_relu(b1,\n",
    "                                 filters=filters,\n",
    "                                 kernel_size=(1, 3),\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 name='conv__7_1_1x3',\n",
    "                                 kernel_initializer=kernel_initializer)\n",
    "\n",
    "            b2 = conv2D_bn_relu(x,\n",
    "                                 filters=filters,\n",
    "                                 kernel_size=(7, 1),\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 name='conv__6_2_7x1',\n",
    "                                 kernel_initializer=kernel_initializer)\n",
    "\n",
    "            b2 = conv2D_bn_relu(b2,\n",
    "                                 filters=filters,\n",
    "                                 kernel_size=(1, 7),\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 name='conv__7_2_1x7',\n",
    "                                 kernel_initializer=kernel_initializer)\n",
    "\n",
    "\n",
    "            b3 = conv2D_bn_relu(x,\n",
    "                                 filters=filters,\n",
    "                                 kernel_size=(5, 1),\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 name='conv__6_3_5x1',\n",
    "                                 kernel_initializer=kernel_initializer)\n",
    "\n",
    "            b3 = conv2D_bn_relu(b3,\n",
    "                                 filters,\n",
    "                                 kernel_size=(1, 5),\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 name='conv__7_3_1x5',\n",
    "                                 kernel_initializer=kernel_initializer)\n",
    "\n",
    "            return layers.concatenate([b1, b2, b3])\n",
    "\n",
    "        inputs = layers.Input(shape=(7**3))\n",
    "        x = Reshape((7, 7, 7), input_shape=(7**3,))(inputs)\n",
    "\n",
    "        x = conv2D_bn_relu(x,\n",
    "                           filters=32,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           name='conv_1_3x3')\n",
    "\n",
    "        x = conv2D_bn_relu(x,\n",
    "                           filters=48,\n",
    "                           kernel_size=(7, 1),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           name='conv_2_7x1')\n",
    "\n",
    "        x = conv2D_bn_relu(x,\n",
    "                           filters=48,\n",
    "                           kernel_size=(1, 7),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           name='conv_3_1x7')\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        x = layers.Dropout(rate=.2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        x = inception_module(x, filters=64)\n",
    "\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        x = layers.Dropout(rate=.2)(x)\n",
    "\n",
    "        x = conv2D_bn_relu(x,\n",
    "                           filters=128,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           name='conv_6_3x3')\n",
    "\n",
    "        x = conv2D_bn_relu(x,\n",
    "                           filters=256,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=1,\n",
    "                           padding='same',\n",
    "                           name='conv_7_3x3')\n",
    "\n",
    "        # x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        x = layers.Dropout(rate=.3)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        x = layers.Dense(units=256, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        x = layers.Dropout(rate=.4)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        x = layers.Dense(units=7**3, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "# Create the model\n",
    "if args.model == \"cnn\":\n",
    "    model = PLSCNNModel(num_layers=0,\n",
    "                        num_hidden=[],\n",
    "                        input_shape=X.shape[1:],\n",
    "                        output_dim=DIM ** 3,\n",
    "                        method=MODEL_TYPE, # \"agnostic\", \"sbrinspiredloss\", \"negative\", \"binary\"\n",
    "                        lmbd=args.lmbd)\n",
    "else:\n",
    "    model = MyModel(num_layers=2,\n",
    "                    num_hidden=[512, 512],\n",
    "                    input_shape=X.shape[1:],\n",
    "                    output_dim=DIM ** 3,\n",
    "                    method=MODEL_TYPE,\n",
    "                    lmbd=args.lmbd)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 343)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_66 (Reshape)            (None, 7, 7, 7)      0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_3x3 (Conv2D)             (None, 7, 7, 32)     2016        reshape_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 7, 7, 32)     128         conv_1_3x3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 7, 7, 32)     0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_7x1 (Conv2D)             (None, 7, 7, 48)     10752       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 7, 7, 48)     192         conv_2_7x1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 7, 7, 48)     0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3_1x7 (Conv2D)             (None, 7, 7, 48)     16128       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 7, 7, 48)     192         conv_3_1x7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 7, 7, 48)     0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling2D) (None, 3, 3, 48)     0           activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 3, 3, 48)     0           max_pooling2d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 3, 3, 48)     192         dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 3, 3, 48)     0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv__6_1_3x1 (Conv2D)          (None, 3, 3, 64)     9216        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv__6_2_7x1 (Conv2D)          (None, 3, 3, 64)     21504       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv__6_3_5x1 (Conv2D)          (None, 3, 3, 64)     15360       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 3, 3, 64)     256         conv__6_1_3x1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 3, 3, 64)     256         conv__6_2_7x1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 3, 3, 64)     256         conv__6_3_5x1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 3, 3, 64)     0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 3, 3, 64)     0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 3, 3, 64)     0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv__7_1_1x3 (Conv2D)          (None, 3, 3, 64)     12288       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv__7_2_1x7 (Conv2D)          (None, 3, 3, 64)     28672       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv__7_3_1x5 (Conv2D)          (None, 3, 3, 64)     20480       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 3, 3, 64)     256         conv__7_1_1x3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 3, 3, 64)     256         conv__7_2_1x7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 3, 3, 64)     256         conv__7_3_1x5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 3, 3, 64)     0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 3, 3, 64)     0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 3, 3, 64)     0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 3, 3, 192)    0           activation_85[0][0]              \n",
      "                                                                 activation_87[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_62 (MaxPooling2D) (None, 1, 1, 192)    0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 1, 1, 192)    0           max_pooling2d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_6_3x3 (Conv2D)             (None, 1, 1, 128)    221184      dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 1, 1, 128)    512         conv_6_3x3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 1, 1, 128)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7_3x3 (Conv2D)             (None, 1, 1, 256)    294912      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 1, 1, 256)    1024        conv_7_3x3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 1, 1, 256)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 1, 1, 256)    0           activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 1, 1, 256)    1024        dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 1, 1, 256)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 256)          0           activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 256)          65792       flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 256)          1024        dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 256)          0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 256)          0           activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 256)          1024        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 256)          0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 343)          88151       activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 813,303\n",
      "Trainable params: 809,879\n",
      "Non-trainable params: 3,424\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002524606C438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.00291563, 0.00291888, 0.00291408, 0.00291801, 0.0029135 ,\n        0.00291563, 0.00291193, 0.00291497, 0.00291011, 0.00291227,\n        0.00291472, 0.0029148 , 0.00291168, 0.00291427, 0.00291475,\n        0.00291408, 0.00291464, 0.00291061, 0.00291461, 0.00291518,\n        0.00291523, 0.0029126 , 0.00291352, 0.00291377, 0.00291267,\n        0.00291365, 0.00291558, 0.00291618, 0.00291657, 0.00291983,\n        0.00291725, 0.00291095, 0.00291596, 0.00291866, 0.00291415,\n        0.00291384, 0.0029145 , 0.00291324, 0.0029154 , 0.00291173,\n        0.00291719, 0.00291358, 0.00291564, 0.0029134 , 0.00291489,\n        0.00291522, 0.00291462, 0.00291264, 0.00291119, 0.00291875,\n        0.002916  , 0.00291573, 0.00291939, 0.00291737, 0.00291073,\n        0.00291621, 0.00291752, 0.00291375, 0.0029172 , 0.00291381,\n        0.0029087 , 0.00291339, 0.0029111 , 0.00291328, 0.0029221 ,\n        0.00291518, 0.0029167 , 0.00291795, 0.0029128 , 0.00291737,\n        0.00291387, 0.00291697, 0.00291162, 0.00291577, 0.00291807,\n        0.00291264, 0.00291691, 0.00291773, 0.00291216, 0.00291828,\n        0.00291461, 0.00291546, 0.00291837, 0.00291362, 0.00291304,\n        0.00291508, 0.00291722, 0.00291649, 0.00291303, 0.00291423,\n        0.00291498, 0.00291295, 0.00291596, 0.00292071, 0.00291707,\n        0.00291304, 0.00291069, 0.00291689, 0.00291512, 0.00291215,\n        0.00291446, 0.00291304, 0.00291721, 0.00291668, 0.00291921,\n        0.00291139, 0.00291522, 0.00291852, 0.00292024, 0.00291356,\n        0.00291616, 0.00291249, 0.00291796, 0.00291166, 0.00291402,\n        0.00291498, 0.00291206, 0.00291501, 0.00291659, 0.00291432,\n        0.00291608, 0.00291639, 0.00291211, 0.00291628, 0.00291463,\n        0.00291825, 0.00291672, 0.00291305, 0.00291695, 0.00291558,\n        0.00291309, 0.00292071, 0.00291412, 0.00291344, 0.00291293,\n        0.00291735, 0.00291603, 0.00291577, 0.00292208, 0.00291682,\n        0.00291458, 0.00291404, 0.00291727, 0.00291334, 0.00291439,\n        0.00291627, 0.0029138 , 0.0029103 , 0.00291282, 0.00291562,\n        0.00291267, 0.00291741, 0.00291778, 0.00291429, 0.00291708,\n        0.00291901, 0.00291274, 0.00291814, 0.00291946, 0.00291857,\n        0.00291528, 0.00291276, 0.00291759, 0.00291813, 0.00291707,\n        0.00291226, 0.00291705, 0.00291412, 0.00291171, 0.00291928,\n        0.00291583, 0.00291761, 0.00291537, 0.00291229, 0.00291671,\n        0.0029189 , 0.00291175, 0.00291625, 0.0029151 , 0.00292011,\n        0.00291531, 0.00291622, 0.0029129 , 0.00291184, 0.00291222,\n        0.0029173 , 0.00291944, 0.0029149 , 0.00291418, 0.00291727,\n        0.00291746, 0.00292046, 0.00291375, 0.00292193, 0.00291757,\n        0.00291161, 0.00291618, 0.00291517, 0.00291711, 0.00291339,\n        0.00291626, 0.00291388, 0.00291754, 0.0029169 , 0.00291779,\n        0.00291791, 0.00291702, 0.00291677, 0.00291538, 0.00291206,\n        0.00291109, 0.00291742, 0.00291177, 0.00291692, 0.00291408,\n        0.00291086, 0.00291708, 0.0029125 , 0.00291407, 0.00291609,\n        0.00291617, 0.00291467, 0.00291838, 0.00291814, 0.00291713,\n        0.0029217 , 0.00291656, 0.00291638, 0.0029166 , 0.00291969,\n        0.00291309, 0.00291404, 0.00291607, 0.00291632, 0.00291537,\n        0.00291774, 0.00291126, 0.00291851, 0.00291731, 0.00291889,\n        0.00291555, 0.00291866, 0.00291533, 0.00291484, 0.00291504,\n        0.00291886, 0.00291603, 0.00291248, 0.00291246, 0.00291792,\n        0.00291951, 0.00291086, 0.00291516, 0.0029176 , 0.00291757,\n        0.00291629, 0.00291662, 0.00291267, 0.00291397, 0.00291411,\n        0.00291514, 0.00291684, 0.00292465, 0.00291403, 0.00291657,\n        0.00291538, 0.00291729, 0.00291733, 0.00291481, 0.0029168 ,\n        0.00291747, 0.00291517, 0.00291694, 0.00291598, 0.00291594,\n        0.00291668, 0.00291588, 0.00291745, 0.00291699, 0.00291672,\n        0.00291498, 0.00291247, 0.00291645, 0.00291641, 0.00291529,\n        0.00291119, 0.0029161 , 0.00291577, 0.00291264, 0.00291254,\n        0.00290949, 0.00291151, 0.00292   , 0.00291768, 0.00291653,\n        0.00291612, 0.00291229, 0.00291733, 0.00291793, 0.00291707,\n        0.00291788, 0.00291639, 0.00291051, 0.00291214, 0.00291453,\n        0.00291791, 0.00291855, 0.00290773, 0.00291822, 0.00291521,\n        0.00291433, 0.00291616, 0.00291645, 0.00291419, 0.00291748,\n        0.00291416, 0.00291649, 0.00291991, 0.00291607, 0.00291358,\n        0.00291028, 0.00291338, 0.00291763, 0.00291402, 0.00291457,\n        0.00291743, 0.00291727, 0.00291542, 0.00290946, 0.00291745,\n        0.00291347, 0.00291846, 0.00291879, 0.00291452, 0.00291836,\n        0.00292028, 0.00291177, 0.00291759, 0.00291678, 0.00291574,\n        0.00291933, 0.00291293, 0.00291267]], dtype=float32)"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[:1].astype(float))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n",
      "Epoch 000: Loss: 5.83772, Accuracy: 0.30713%\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "if TRAIN:\n",
    "    history = model.train(EPOCHS,\n",
    "                          dataset,\n",
    "                          \"models/{}\".format(model_name) + str(np.random.randint(1, 10000)),\n",
    "                          DIM,\n",
    "                          validation_set,\n",
    "                          args.use_prop,\n",
    "                          args.patience)\n",
    "\n",
    "    for name in history.keys():\n",
    "        values = history[name]\n",
    "\n",
    "        plt.plot(np.arange(0, len(values)), values,\n",
    "                 label=name)\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.legend()\n",
    "        plt.savefig(\"{}/{}.png\".format(SAVE_PATH, name))\n",
    "        plt.close()\n",
    "\n",
    "        with open(\"{}/{}.csv\".format(SAVE_PATH, name), \"w\") as file:\n",
    "            wr = csv.writer(file)\n",
    "            wr.writerow(values)\n",
    "            file.close()\n",
    "    exit(0)\n",
    "\n",
    "else:\n",
    "    model.model = tf.saved_model.load(\"models/{}\".format(model_name))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# Test the model\n",
    "\n",
    "# Make predictions\n",
    "tensor_X = X.astype(np.float32)\n",
    "predict_val = model.predict_from_saved_model(tensor_X).numpy()\n",
    "\n",
    "# Prune values according to constraints propagator if required\n",
    "if args.use_prop:\n",
    "    predict_val *= (1 - P)\n",
    "\n",
    "# Count of correct predictions grouped by number of assigned variables\n",
    "pred_by_num_assigned = np.zeros(shape=(DIM ** 2))\n",
    "# Count of feasible solutions grouped by number of assigned variables\n",
    "feas_by_num_assigned = np.zeros(shape=(DIM ** 2))\n",
    "# Count of total examples grouped by number of assigned variables\n",
    "tot_by_num_assigned = np.zeros(shape=(DIM ** 2))\n",
    "# Count of random correct predictions grouped by number of assigned variables\n",
    "rand_pred_by_num_assigned = np.zeros(shape=(DIM ** 2))\n",
    "# Count of random feasible solutions grouped by number of assigned variables\n",
    "rand_feas_by_num_assigned = np.zeros(shape=(DIM ** 2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute overall accuracy on training set\n",
    "acc = 0\n",
    "count = 0\n",
    "acc_rand = 0\n",
    "\n",
    "# Compute accuracy grouped by number of assigned variables\n",
    "preds = []\n",
    "for x, pred, y, d in zip(X, predict_val, Y, P):\n",
    "\n",
    "    if count % 1000 == 0:\n",
    "        print(\"Examined {} instances\".format(count))\n",
    "\n",
    "    num_assigned_vars = np.sum(x.astype(np.int8))\n",
    "    pred_label = np.argmax(pred.reshape(-1))\n",
    "    correct_label = np.argmax(y.reshape(-1))\n",
    "\n",
    "    if pred_label == correct_label:\n",
    "        acc += 1\n",
    "        pred_by_num_assigned[num_assigned_vars] += 1\n",
    "\n",
    "    # Create a problem instance with current examples for net prediction\n",
    "    square = np.reshape(x, (DIM, DIM, DIM))\n",
    "    pls = PLSInstance(n=DIM)\n",
    "    pls.square = square.copy()\n",
    "    # assert pls.__check_constraints__(), \"Constraints should be verified before assignment\"\n",
    "\n",
    "    # Make the prediction assignment\n",
    "    assignment = np.argmax(pred)\n",
    "    assignment = np.unravel_index(assignment, shape=(DIM, DIM, DIM))\n",
    "\n",
    "    # Local consistency\n",
    "    local_feas = pls.assign(assignment[0], assignment[1], assignment[2])\n",
    "\n",
    "    '''vals_square = np.argmax(square, axis=2) + np.sum(square, axis=2)\n",
    "    solver = utility.PLSSolver(DIM, square=np.reshape(vals_square, -1))\n",
    "    res = solver.solve()\n",
    "    assert res, \"Constraint solver is wrong because the input comes from a real solution\"'''\n",
    "\n",
    "    # Global consistency\n",
    "    if local_feas:\n",
    "        vals_square = np.argmax(pls.square.copy(), axis=2) + np.sum(pls.square.copy(), axis=2)\n",
    "        solver = PLSSolver(DIM, square=np.reshape(vals_square, -1))\n",
    "        feas = solver.solve()\n",
    "    else:\n",
    "        feas = local_feas\n",
    "\n",
    "    if feas:\n",
    "        feas_by_num_assigned[num_assigned_vars] += 1\n",
    "\n",
    "    # Check random assignment performance if required\n",
    "    if args.rnd_feas:\n",
    "        if not args.use_prop:\n",
    "            d = None\n",
    "        rand_assignment = random_assigner(DIM ** 3, d)\n",
    "        if rand_assignment == correct_label:\n",
    "            acc_rand += 1\n",
    "            rand_pred_by_num_assigned[num_assigned_vars] += 1\n",
    "\n",
    "        # Create a problem instance with current training example for random prediction\n",
    "        square = np.reshape(x, (DIM, DIM, DIM))\n",
    "        pls = PLSInstance(n=DIM)\n",
    "        pls.square = square.copy()\n",
    "        # assert pls.__check_constraints__(), \"Constraints should be verified before assignment\"\n",
    "\n",
    "        # Make the random assignment\n",
    "        rand_assignment = np.unravel_index(rand_assignment, shape=(DIM, DIM, DIM))\n",
    "\n",
    "        local_feas = pls.assign(rand_assignment[0], rand_assignment[1], rand_assignment[2])\n",
    "\n",
    "        # Check global consistency\n",
    "        if local_feas:\n",
    "            vals_square = np.argmax(pls.square.copy(), axis=2) + np.sum(pls.square.copy(), axis=2)\n",
    "            solver = PLSSolver(DIM, square=np.reshape(vals_square, -1))\n",
    "            feas = solver.solve()\n",
    "        else:\n",
    "            feas = local_feas\n",
    "\n",
    "        if feas:\n",
    "            rand_feas_by_num_assigned[num_assigned_vars] += 1\n",
    "\n",
    "    # Increase count of solutions with this number of assignments\n",
    "    tot_by_num_assigned[num_assigned_vars] += 1\n",
    "    count += 1\n",
    "\n",
    "    # Save results checkpoint\n",
    "    if count % 1000 == 0:\n",
    "\n",
    "        feasibility = list((feas_by_num_assigned / (tot_by_num_assigned + 1e-8))[1:])\n",
    "\n",
    "        if not args.use_prop:\n",
    "            filename = \"{}/feasibility_{}.csv\".format(SAVE_PATH, mode)\n",
    "        else:\n",
    "            if args.leave_columns_domains:\n",
    "                filename = \"{}/feasibility_{}_with_row_prop.csv\".format(SAVE_PATH, mode)\n",
    "            else:\n",
    "                filename = \"{}/feasibility_{}_with_full_prop.csv\".format(SAVE_PATH, mode)\n",
    "\n",
    "        with open(filename, \"w\") as epoch_file:\n",
    "            wr = csv.writer(epoch_file)\n",
    "            wr.writerow(feasibility)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check accuracy is correctly computed\n",
    "assert np.sum(pred_by_num_assigned) == acc and np.sum(tot_by_num_assigned) == count, \\\n",
    "    \"acc: {} | acc_vectorized: {} | count: {} | count_vectorized: {}\".format(acc, np.sum(pred_by_num_assigned),\n",
    "                                                                             count, np.sum(tot_by_num_assigned))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make plots\n",
    "\n",
    "accuracy = list((pred_by_num_assigned / (tot_by_num_assigned + 1e-8))[1:])\n",
    "feasibility = list((feas_by_num_assigned / (tot_by_num_assigned + 1e-8))[1:])\n",
    "if args.rnd_feas:\n",
    "    random_feasibility = list((rand_feas_by_num_assigned / (tot_by_num_assigned + 1e-8))[1:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save random assigner results\n",
    "if args.rnd_feas:\n",
    "    RANDOM_SAVE_PATH = \"plots/test-pls-{}-tf-keras/random/\".format(DIM)\n",
    "\n",
    "    if args.use_prop:\n",
    "        if not args.leave_columns_domains:\n",
    "            RANDOM_SAVE_PATH += \"rows-and-columns-prop\"\n",
    "        else:\n",
    "            RANDOM_SAVE_PATH += \"rows-prop\"\n",
    "    else:\n",
    "        RANDOM_SAVE_PATH += \"no-prop\"\n",
    "\n",
    "    try:\n",
    "        os.makedirs(RANDOM_SAVE_PATH)\n",
    "    except:\n",
    "        print(\"Directory {} already exists\".format(RANDOM_SAVE_PATH))\n",
    "\n",
    "    with open(\"{}/random_feasibility.csv\".format(RANDOM_SAVE_PATH, mode), \"w\") as epoch_file:\n",
    "        wr = csv.writer(epoch_file)\n",
    "        wr.writerow(random_feasibility)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}